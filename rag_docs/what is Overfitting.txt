Overfitting is a common problem found during model training, where a model becomes too aligned and specialized with the training data.The model performs well on training data but poorly on new data. The reason is during training, the training algorithm aims to optimize the training metric such as R-squared using the training data. In this pursuit, it might capture irrelevant details and become too complex. There is a common technique to prevent overfitting and build a generalized model. It involves train-test-split and cross-validation. First we split the data into train and test sets (usually 80%-20%). The test set remains ‘unseen’  throughout the process. After that we further split the training data into k folds (e.g., 5 folds for 5-fold cross-validation). In each fold, use k-1 folds for training and the remaining fold for validation. Repeat this process for all folds. Now we can use the cross-validation results to evaluate the model's performance across different data subsets. Inconsistent results across folds might indicate overfitting. Finally, we train the final model using the entire training data based on the best configuration identified during cross-validation, and check result using the unseen test data set